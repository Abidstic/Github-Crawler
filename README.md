# ðŸš€ GitHub Unified Crawler

A comprehensive, intelligent GitHub repository crawler with real-time progress tracking, smart rate limiting, and automatic resume capabilities.

## âœ¨ Features

-   **ðŸŽ¯ Single Command**: Crawl entire repository with one command
-   **âš¡ Async & Parallel**: Intelligent parallel processing where possible
-   **ðŸ›¡ï¸ Smart Rate Limiting**: Automatic GitHub API rate limit management
-   **ðŸ“Š Real-time Progress**: Live CLI progress display with ETA
-   **ðŸ’¾ Resume Capability**: Automatic checkpointing and resume from failures
-   **ðŸ”„ Fault Tolerant**: Handles network errors, API failures, and interruptions
-   **ðŸ“ˆ Comprehensive Data**: PRs, commits, reviews, files, comments, and detailed commit info

## ðŸ—ï¸ What It Crawls

1. **Pull Requests** - All PRs with metadata
2. **Repository Commits** - All commits in the repository
3. **PR Files** - Files changed in each PR
4. **PR Reviews** - Reviews for each PR
5. **PR Commits** - Commits within each PR
6. **PR Comments** - Review comments for each PR
7. **Single Commits** - Detailed data for each unique commit

## ðŸ“¦ Installation

1. **Clone or download the project files**

2. **Install dependencies:**

```bash
pip install -r requirements.txt
```

3. **Set up GitHub token:**

```bash
# Get a GitHub personal access token with 'repo' permissions
# From: GitHub Settings â†’ Developer settings â†’ Personal access tokens

export GH_TOKEN='your_github_token_here'
```

## ðŸš€ Quick Start

**Note: If python doesnâ€™t work on your system, try using python3 instead.**

### Basic Usage

```bash
# Crawl complete repository
python main.py --owner facebook --repo react

# Crawl without resuming from previous attempts
python main.py --owner microsoft --repo vscode --no-resume

# Use conservative rate limiting (recommended for large repos)
python main.py --owner tensorflow --repo tensorflow --conservative
```

### Advanced Options

```bash
# Validate existing crawled data
python main.py --owner facebook --repo react --validate-only

# Show usage examples
python main.py --examples

# Custom concurrent request limit
python main.py --owner myorg --repo myrepo --max-concurrent 5
```

## ðŸ“Š Real-time Progress Display

While running, you'll see a live updating display:

```
================================================================================
ðŸš€ GitHub Crawler - facebook/react
â±ï¸  Total Runtime: 45m 23s
ðŸ”„ Current: Processing commit batches batch 15/87
================================================================================
ðŸŸ¢ Rate Limit: 3247/5000 (Reset: 14:30:00) - 35.1% used

âœ… Pull Requests
   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0%
   Progress: 2,431/2,431 (Failed: 0, Skipped: 0)
   Rate: 54.2/min | Duration: 44m 52s | ETA: N/A

ðŸ”„ Single Commits
   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 62.3%
   Progress: 9,847/15,806 (Failed: 12, Skipped: 0)
   Rate: 218.5/min | Duration: 45m 06s | ETA: 27m 18s

ðŸ“Š Overall Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 71.2%
   47,329/66,498 total items
================================================================================
Press Ctrl+C to stop gracefully
```

## ðŸ“ Output Structure

```
crawled-data/
â””â”€â”€ {owner}-{repo}/
    â”œâ”€â”€ pull/
    â”‚   â”œâ”€â”€ all_data.json                    # All pull requests
    â”‚   â””â”€â”€ {pr_number}/
    â”‚       â”œâ”€â”€ files/all_data.json          # PR files
    â”‚       â”œâ”€â”€ reviews/all_data.json        # PR reviews
    â”‚       â”œâ”€â”€ commits/all_data.json        # PR commits
    â”‚       â””â”€â”€ comments/all_data.json       # PR comments
    â”œâ”€â”€ commit/
    â”‚   â”œâ”€â”€ all_data.json                    # All repository commits
    â”‚   â””â”€â”€ all/
    â”‚       â””â”€â”€ {sha}.json                   # Individual commit details
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ unified_crawler_{timestamp}.log  # Detailed logs
    â””â”€â”€ .checkpoint.json                     # Resume checkpoint (auto-deleted when complete)
```

## ðŸ›¡ï¸ Rate Limiting Intelligence

The crawler automatically manages GitHub's API rate limits:

-   **5,000 requests/hour** for authenticated users
-   **Smart throttling** based on remaining quota
-   **Conservative mode** after rate limit hits
-   **Automatic waiting** with progress display
-   **Optimal batching** for parallel requests

### Rate Limit Strategies:

-   **Green (>1000 remaining)**: Full speed parallel processing
-   **Yellow (200-1000 remaining)**: Moderate throttling
-   **Red (<200 remaining)**: Conservative mode with delays
-   **Exhausted**: Automatic wait until reset with countdown

## ðŸ’¾ Resume Capability

The crawler automatically saves progress and can resume from any point:

```bash
# If interrupted, simply run the same command again
python main.py --owner facebook --repo react

# The crawler will automatically:
# âœ… Skip completed crawlers
# âœ… Resume incomplete operations
# âœ… Continue from last checkpoint
```

## ðŸ”§ Configuration

Modify `config.py` to customize behavior:

```python
# Rate limiting
max_requests_per_hour = 4800    # Conservative limit
rate_limit_buffer = 200         # Safety buffer

# Performance
max_concurrent_requests = 10    # Parallel requests
items_per_page = 100           # GitHub API max

# Retry behavior
max_retries = 3                # Request retries
base_backoff_delay = 60.0      # Base delay seconds
```

## ðŸš¨ Error Handling

The crawler handles various scenarios gracefully:

-   **Network failures**: Automatic retry with exponential backoff
-   **Rate limit exceeded**: Smart waiting with progress display
-   **API errors**: Detailed logging and graceful continuation
-   **Interruptions**: Save checkpoint and resume capability
-   **Invalid data**: Skip and continue with error logging

## ðŸ“‹ Validation

Validate your crawled data:

```bash
python main.py --owner facebook --repo react --validate-only
```

This checks:

-   JSON file integrity
-   Expected folder structure
-   Data completeness
-   File counts and sizes

## ðŸŽ¯ Integration with Reviewer Recommender

After crawling, your data structure is ready for the reviewer recommendation system:

```python
# In your reviewer recommender
CRAWLED_DATA_PATH = "crawled-data/facebook-react"

# Access the data:
# - Pull requests: {CRAWLED_DATA_PATH}/pull/all_data.json
# - Commits: {CRAWLED_DATA_PATH}/commit/all_data.json
# - Individual commits: {CRAWLED_DATA_PATH}/commit/all/{sha}.json
# - PR dependencies: {CRAWLED_DATA_PATH}/pull/{pr_number}/{type}/all_data.json
```

## ðŸ› Troubleshooting

### Common Issues:

**1. Rate Limit Errors**

```bash
# Use conservative mode
python main.py --owner repo --repo name --conservative
```

**2. Large Repository Timeouts**

```bash
# Reduce concurrent requests
python main.py --owner repo --repo name --max-concurrent 3
```

**3. Interrupted Crawl**

```bash
# Simply rerun - it will resume automatically
python main.py --owner repo --repo name
```

**4. Missing Token**

```bash
# Set GitHub token
export GH_TOKEN='ghp_your_token_here'
```

### Getting Help:

1. Run with `--examples` for usage examples
2. Check log files in `crawled-data/{owner}-{repo}/logs/`
3. Use `--validate-only` to check data integrity

## ðŸ“ˆ Performance Tips

-   **Large repositories**: Use `--conservative` mode
-   **Fast networks**: Increase `--max-concurrent`
-   **Rate limit issues**: The crawler handles this automatically
-   **Resume interrupted crawls**: Just rerun the same command

## ðŸ”’ Security

-   Token is read from environment variable only
-   No token storage in code or files
-   Respects GitHub's rate limits and terms of service
